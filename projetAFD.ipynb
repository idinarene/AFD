{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.optimizers import SGD\n",
    "from keras import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "var_w=lambda shape: initializers.RandomUniform(-0.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentences  words  letters.all  syllables  punct  avg.sentc.length  \\\n",
      "0           16.0  233.0       1116.0      363.0   32.0         14.562500   \n",
      "1            7.0  180.0        866.0      268.0   18.0         25.714286   \n",
      "2           10.0  180.0        861.0      269.0   20.0         18.000000   \n",
      "3           10.0  180.0        872.0      281.0   20.0         18.000000   \n",
      "4           11.0  187.0        849.0      283.0   29.0         17.000000   \n",
      "5            9.0  180.0        857.0      272.0   24.0         20.000000   \n",
      "6            5.0  130.0        607.0      197.0   15.0         26.000000   \n",
      "7            9.0  183.0        717.0      229.0   24.0         20.333333   \n",
      "8           11.0  185.0        788.0      251.0   42.0         16.818182   \n",
      "9            9.0  157.0        641.0      208.0   20.0         17.444444   \n",
      "10           5.0  104.0        440.0      148.0   12.0         20.800000   \n",
      "11          21.0  377.0       1698.0      562.0   41.0         17.952381   \n",
      "12           9.0  191.0        797.0      265.0   23.0         21.222222   \n",
      "13           8.0  155.0        732.0      248.0   13.0         19.375000   \n",
      "14          13.0  173.0        734.0      237.0   22.0         13.307692   \n",
      "15           4.0  121.0        519.0      166.0   13.0         30.250000   \n",
      "16          10.0  151.0        724.0      249.0   16.0         15.100000   \n",
      "17          10.0  180.0        754.0      248.0   30.0         18.000000   \n",
      "18           9.0  150.0        658.0      230.0   10.0         16.666667   \n",
      "19          10.0  167.0        774.0      252.0   24.0         16.700000   \n",
      "20           7.0  148.0        719.0      234.0   10.0         21.142857   \n",
      "21           6.0   80.0        341.0      111.0   14.0         13.333333   \n",
      "22          12.0  182.0        921.0      297.0   21.0         15.166667   \n",
      "23          10.0  207.0        860.0      288.0   30.0         20.700000   \n",
      "24           7.0  174.0        748.0      238.0   25.0         24.857143   \n",
      "25           8.0  171.0        667.0      213.0   19.0         21.375000   \n",
      "26          11.0  162.0        712.0      230.0   24.0         14.727273   \n",
      "27           9.0  153.0        742.0      231.0   16.0         17.000000   \n",
      "28          10.0  213.0        906.0      307.0   32.0         21.300000   \n",
      "29          12.0  177.0        874.0      304.0   21.0         14.750000   \n",
      "...          ...    ...          ...        ...    ...               ...   \n",
      "27280        2.0   21.0         77.0       24.0    3.0         10.500000   \n",
      "27281        3.0   42.0        145.0       54.0   11.0         14.000000   \n",
      "27282        5.0   14.0         49.0       15.0    7.0          2.800000   \n",
      "27283        7.0   38.0        126.0       45.0   13.0          5.428571   \n",
      "27284        3.0   45.0        168.0       57.0    6.0         15.000000   \n",
      "27285        6.0   39.0        167.0       52.0    9.0          6.500000   \n",
      "27286        6.0   41.0        169.0       53.0    7.0          6.833333   \n",
      "27287        3.0   28.0        142.0       38.0    4.0          9.333333   \n",
      "27288        6.0   54.0        237.0       88.0    9.0          9.000000   \n",
      "27289        8.0   29.0         92.0       39.0   12.0          3.625000   \n",
      "27290        4.0   54.0        193.0       66.0    9.0         13.500000   \n",
      "27291        1.0   27.0        101.0       32.0    5.0         27.000000   \n",
      "27292        3.0   24.0         96.0       28.0    4.0          8.000000   \n",
      "27293        5.0   55.0        230.0       78.0    8.0         11.000000   \n",
      "27294        3.0   40.0        150.0       49.0    6.0         13.333333   \n",
      "27295        2.0   32.0        137.0       38.0    6.0         16.000000   \n",
      "27296        6.0   40.0        128.0       44.0   11.0          6.666667   \n",
      "27297        1.0   25.0        115.0       37.0    5.0         25.000000   \n",
      "27298        8.0  100.0        410.0      133.0   13.0         12.500000   \n",
      "27299        9.0   37.0        148.0       44.0   13.0          4.111111   \n",
      "27300        8.0  108.0        447.0      150.0   19.0         13.500000   \n",
      "27301        4.0   24.0         80.0       27.0    8.0          6.000000   \n",
      "27302        8.0   39.0        138.0       49.0   17.0          4.875000   \n",
      "27303        5.0   68.0        256.0       81.0   11.0         13.600000   \n",
      "27304        3.0   14.0         63.0       20.0    6.0          4.666667   \n",
      "27305        8.0   76.0        339.0      118.0   15.0          9.500000   \n",
      "27306        2.0   27.0        115.0       34.0    5.0         13.500000   \n",
      "27307        2.0   20.0         63.0       23.0    4.0         10.000000   \n",
      "27308        5.0   27.0        107.0       37.0    8.0          5.400000   \n",
      "27309        1.0   38.0        174.0       52.0    7.0         38.000000   \n",
      "\n",
      "       avg.word.length  avg.syll.word  sntc.per.word       TTR    ...     \\\n",
      "0             4.789700       1.557940       0.068670  0.587983    ...      \n",
      "1             4.811111       1.488889       0.038889  0.638889    ...      \n",
      "2             4.783333       1.494444       0.055556  0.716667    ...      \n",
      "3             4.844444       1.561111       0.055556  0.677778    ...      \n",
      "4             4.540107       1.513369       0.058824  0.582888    ...      \n",
      "5             4.761111       1.511111       0.050000  0.644444    ...      \n",
      "6             4.669231       1.515385       0.038462  0.684615    ...      \n",
      "7             3.918033       1.251366       0.049180  0.508197    ...      \n",
      "8             4.259459       1.356757       0.059459  0.675676    ...      \n",
      "9             4.082803       1.324841       0.057325  0.598726    ...      \n",
      "10            4.230769       1.423077       0.048077  0.769231    ...      \n",
      "11            4.503979       1.490716       0.055703  0.506631    ...      \n",
      "12            4.172775       1.387435       0.047120  0.649215    ...      \n",
      "13            4.722581       1.600000       0.051613  0.529032    ...      \n",
      "14            4.242775       1.369942       0.075145  0.676301    ...      \n",
      "15            4.289256       1.371901       0.033058  0.669421    ...      \n",
      "16            4.794702       1.649007       0.066225  0.615894    ...      \n",
      "17            4.188889       1.377778       0.055556  0.661111    ...      \n",
      "18            4.386667       1.533333       0.060000  0.573333    ...      \n",
      "19            4.634731       1.508982       0.059880  0.682635    ...      \n",
      "20            4.858108       1.581081       0.047297  0.601351    ...      \n",
      "21            4.262500       1.387500       0.075000  0.812500    ...      \n",
      "22            5.060440       1.631868       0.065934  0.692308    ...      \n",
      "23            4.154589       1.391304       0.048309  0.613527    ...      \n",
      "24            4.298851       1.367816       0.040230  0.637931    ...      \n",
      "25            3.900585       1.245614       0.046784  0.567251    ...      \n",
      "26            4.395062       1.419753       0.067901  0.635802    ...      \n",
      "27            4.849673       1.509804       0.058824  0.607843    ...      \n",
      "28            4.253521       1.441315       0.046948  0.619718    ...      \n",
      "29            4.937853       1.717514       0.067797  0.638418    ...      \n",
      "...                ...            ...            ...       ...    ...      \n",
      "27280         3.666667       1.142857       0.095238  0.714286    ...      \n",
      "27281         3.452381       1.285714       0.071429  0.714286    ...      \n",
      "27282         3.500000       1.071429       0.357143  0.928571    ...      \n",
      "27283         3.315789       1.184211       0.184211  0.789474    ...      \n",
      "27284         3.733333       1.266667       0.066667  0.688889    ...      \n",
      "27285         4.282051       1.333333       0.153846  0.794872    ...      \n",
      "27286         4.121951       1.292683       0.146341  0.829268    ...      \n",
      "27287         5.071429       1.357143       0.107143  0.821429    ...      \n",
      "27288         4.388889       1.629630       0.111111  0.814815    ...      \n",
      "27289         3.172414       1.344828       0.275862  0.655172    ...      \n",
      "27290         3.574074       1.222222       0.074074  0.685185    ...      \n",
      "27291         3.740741       1.185185       0.037037  0.666667    ...      \n",
      "27292         4.000000       1.166667       0.125000  0.791667    ...      \n",
      "27293         4.181818       1.418182       0.090909  0.727273    ...      \n",
      "27294         3.750000       1.225000       0.075000  0.825000    ...      \n",
      "27295         4.281250       1.187500       0.062500  0.781250    ...      \n",
      "27296         3.200000       1.100000       0.150000  0.675000    ...      \n",
      "27297         4.600000       1.480000       0.040000  0.800000    ...      \n",
      "27298         4.100000       1.330000       0.080000  0.710000    ...      \n",
      "27299         4.000000       1.189189       0.243243  0.648649    ...      \n",
      "27300         4.138889       1.388889       0.074074  0.620370    ...      \n",
      "27301         3.333333       1.125000       0.166667  0.750000    ...      \n",
      "27302         3.538462       1.256410       0.205128  0.717949    ...      \n",
      "27303         3.764706       1.191176       0.073529  0.647059    ...      \n",
      "27304         4.500000       1.428571       0.214286  0.928571    ...      \n",
      "27305         4.460526       1.552632       0.105263  0.684211    ...      \n",
      "27306         4.259259       1.259259       0.074074  0.888889    ...      \n",
      "27307         3.150000       1.150000       0.100000  0.900000    ...      \n",
      "27308         3.962963       1.370370       0.185185  0.851852    ...      \n",
      "27309         4.578947       1.368421       0.026316  0.710526    ...      \n",
      "\n",
      "       Maas a  Maas lgV0  MATTR  MSTTR    MTLD  Root TTR  Summer  TTR.1  \\\n",
      "0        0.20       4.96   0.72   0.70   70.74      8.98    0.88   0.59   \n",
      "1        0.20       5.07   0.74   0.76  107.12      8.57    0.89   0.64   \n",
      "2        0.17       5.99   0.77   0.78  161.93      9.62    0.92   0.72   \n",
      "3        0.18       5.49   0.77   0.81  141.10      9.09    0.90   0.68   \n",
      "4        0.21       4.61   0.68   0.64   70.69      7.97    0.87   0.58   \n",
      "5        0.19       5.13   0.71   0.73  100.72      8.65    0.89   0.64   \n",
      "6        0.19       5.04   0.73   0.73   88.86      7.81    0.89   0.68   \n",
      "7        0.24       3.99   0.62   0.60   41.25      6.87    0.83   0.51   \n",
      "8        0.18       5.52   0.73   0.78  111.92      9.19    0.90   0.68   \n",
      "9        0.21       4.50   0.66   0.63   53.33      7.50    0.86   0.60   \n",
      "10       0.17       5.74   0.78   0.77   79.10      7.84    0.92   0.77   \n",
      "11       0.21       4.91   0.70   0.67   78.70      9.84    0.87   0.51   \n",
      "12       0.19       5.27   0.75   0.72   99.84      8.97    0.90   0.65   \n",
      "13       0.24       3.93   0.61   0.57   38.88      6.59    0.83   0.53   \n",
      "14       0.18       5.41   0.70   0.71   98.02      8.90    0.90   0.68   \n",
      "15       0.20       4.77   0.71   0.68   69.57      7.36    0.88   0.67   \n",
      "16       0.21       4.59   0.68   0.67   60.40      7.57    0.87   0.62   \n",
      "17       0.19       5.31   0.74   0.78  108.83      8.87    0.90   0.66   \n",
      "18       0.23       4.22   0.62   0.66   64.92      7.02    0.85   0.57   \n",
      "19       0.18       5.43   0.74   0.72   91.80      8.82    0.90   0.68   \n",
      "20       0.22       4.44   0.65   0.65   54.32      7.32    0.86   0.60   \n",
      "21       0.16       5.96    NaN    NaN  119.47      7.27    0.92   0.81   \n",
      "22       0.18       5.69   0.78   0.84  126.80      9.34    0.91   0.69   \n",
      "23       0.20       5.03   0.74   0.74   85.86      8.83    0.89   0.61   \n",
      "24       0.20       5.01   0.68   0.73   78.48      8.41    0.89   0.64   \n",
      "25       0.22       4.35   0.64   0.66   58.46      7.42    0.85   0.57   \n",
      "26       0.20       4.88   0.72   0.71   81.74      8.09    0.88   0.64   \n",
      "27       0.21       4.54   0.71   0.71   78.37      7.52    0.87   0.61   \n",
      "28       0.20       5.14   0.72   0.74   99.98      9.04    0.89   0.62   \n",
      "29       0.20       5.04   0.72   0.75   84.48      8.49    0.89   0.64   \n",
      "...       ...        ...    ...    ...     ...       ...     ...    ...   \n",
      "27280    0.29       2.57    NaN    NaN   21.00      3.27    0.58   0.71   \n",
      "27281    0.24       3.56    NaN    NaN   30.07      4.63    0.81   0.71   \n",
      "27282    0.16       4.73    NaN    NaN   54.88      3.47    0.79   0.93   \n",
      "27283    0.20       4.17    NaN    NaN   50.54      4.87    0.85   0.79   \n",
      "27284    0.24       3.46    NaN    NaN   45.00      4.62    0.80   0.69   \n",
      "27285    0.20       4.28    NaN    NaN   53.23      4.96    0.86   0.79   \n",
      "27286    0.18       4.89    NaN    NaN   67.24      5.31    0.89   0.83   \n",
      "27287    0.20       4.02    NaN    NaN   43.90      4.35    0.84   0.82   \n",
      "27288    0.17       5.20    NaN    NaN   81.65      5.99    0.90   0.81   \n",
      "27289    0.29       2.64    NaN    NaN   19.33      3.53    0.65   0.66   \n",
      "27290    0.23       3.69    NaN    NaN   35.88      5.04    0.82   0.69   \n",
      "27291    0.29       2.61    NaN    NaN   21.51      3.46    0.63   0.67   \n",
      "27292    0.23       3.40    NaN    NaN   32.26      3.88    0.76   0.79   \n",
      "27293    0.21       4.10    NaN    NaN   55.72      5.39    0.85   0.73   \n",
      "27294    0.18       4.76    NaN    NaN   64.00      5.22    0.89   0.82   \n",
      "27295    0.22       3.77    NaN    NaN   40.96      4.42    0.82   0.78   \n",
      "27296    0.26       3.19    NaN    NaN   19.92      4.27    0.76   0.68   \n",
      "27297    0.22       3.56    NaN    NaN   35.00      4.00    0.79   0.80   \n",
      "27298    0.19       4.89    NaN   0.71   86.56      7.10    0.89   0.71   \n",
      "27299    0.28       2.91    NaN    NaN   23.32      3.95    0.72   0.65   \n",
      "27300    0.22       4.15   0.63   0.65   60.15      6.45    0.85   0.62   \n",
      "27301    0.26       3.02    NaN    NaN   26.88      3.67    0.71   0.75   \n",
      "27302    0.24       3.48    NaN    NaN   30.06      4.48    0.80   0.72   \n",
      "27303    0.24       3.72    NaN    NaN   57.70      5.34    0.82   0.65   \n",
      "27304    0.16       4.73    NaN    NaN   54.88      3.47    0.79   0.93   \n",
      "27305    0.22       4.19    NaN    NaN   56.71      5.96    0.85   0.68   \n",
      "27306    0.16       5.21    NaN    NaN   68.04      4.62    0.90   0.89   \n",
      "27307    0.16       4.78    NaN    NaN   56.00      4.02    0.86   0.90   \n",
      "27308    0.18       4.42    NaN    NaN   51.03      4.43    0.86   0.85   \n",
      "27309    0.24       3.38    NaN    NaN   21.14      4.38    0.78   0.71   \n",
      "\n",
      "       Uber index  Yule's K  \n",
      "0           24.30    146.62  \n",
      "1           26.14    141.98  \n",
      "2           35.15     62.96  \n",
      "3           30.11     70.99  \n",
      "4           22.02    124.11  \n",
      "5           26.66     91.98  \n",
      "6           27.16    111.24  \n",
      "7           17.41    195.29  \n",
      "8           30.19    138.50  \n",
      "9           21.65    129.01  \n",
      "10          35.71     77.66  \n",
      "11          22.48    133.40  \n",
      "12          27.73    114.58  \n",
      "13          17.35    322.16  \n",
      "14          29.49     94.89  \n",
      "15          24.89    101.09  \n",
      "16          22.56    118.42  \n",
      "17          28.30     80.86  \n",
      "18          19.60    170.67  \n",
      "19          29.80    149.16  \n",
      "20          21.32    209.09  \n",
      "21          40.16     62.50  \n",
      "22          31.98     76.68  \n",
      "23          25.28     87.28  \n",
      "24          25.71    154.58  \n",
      "25          20.25    142.95  \n",
      "26          24.82    103.64  \n",
      "27          22.08    111.92  \n",
      "28          26.09     96.98  \n",
      "29          25.93     94.48  \n",
      "...           ...       ...  \n",
      "27280       11.96    317.46  \n",
      "27281       18.03    351.47  \n",
      "27282       40.81    102.04  \n",
      "27283       24.31    152.35  \n",
      "27284       16.89    187.65  \n",
      "27285       25.39    131.49  \n",
      "27286       31.99    107.08  \n",
      "27287       24.51    178.57  \n",
      "27288       33.74    137.17  \n",
      "27289       11.65    499.41  \n",
      "27290       18.28    322.36  \n",
      "27291       11.63    493.83  \n",
      "27292       18.78    173.61  \n",
      "27293       21.90    132.23  \n",
      "27294       30.72    100.00  \n",
      "27295       21.13    253.91  \n",
      "27296       15.04    375.00  \n",
      "27297       20.17    256.00  \n",
      "27298       26.89     88.00  \n",
      "27299       13.08    262.97  \n",
      "27300       19.94    121.74  \n",
      "27301       15.25    416.67  \n",
      "27302       17.59    276.13  \n",
      "27303       17.76    177.34  \n",
      "27304       40.81    102.04  \n",
      "27305       21.46    211.22  \n",
      "27306       40.05    109.74  \n",
      "27307       36.99    100.00  \n",
      "27308       29.42    137.17  \n",
      "27309       16.82    221.61  \n",
      "\n",
      "[27310 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# load pima indians dataset\n",
    "dataset = pd.read_csv(\"/Users/malush/Desktop/projet AFD 2018/train_cap2018.csv\", sep=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "scaler = StandardScaler()\n",
    "X=dataset.iloc[:,1:59].astype(float)\n",
    "Y=dataset.iloc[:,59]\n",
    "print(X)\n",
    "\n",
    "#X, X_test, Y, Y_test = train_test_split(X, Y)\n",
    "w = np.isnan(X)\n",
    "X[w] = 0#X.mean() à changer\n",
    "w2= np.isnan(X_test)\n",
    "X_test[w2] = 0\n",
    "\n",
    "#Normalisation \n",
    "\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27310,)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 ... 0 0 0]\n",
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Pour encoder Y\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "#converti le Y de chaine de caractères à numérique\n",
    "Y_numerique = encoder.transform(Y)\n",
    "\n",
    "print(Y_numerique)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_encode_binaire = np_utils.to_categorical(Y_numerique)#6\n",
    "\n",
    "print(y_encode_binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_encode_binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.76657574,  3.93245254,  4.54008689, ..., -1.31592026,\n",
       "        -0.10833058, -0.29768432],\n",
       "       [ 0.23533113,  2.65752488,  3.17149855, ..., -0.82367704,\n",
       "         0.02625278, -0.34571468],\n",
       "       [ 1.07907934,  2.65752488,  3.14412678, ..., -0.0360879 ,\n",
       "         0.6852724 , -1.16367995],\n",
       "       ...,\n",
       "       [-1.17091587, -1.19131333, -1.22440721, ...,  1.73598767,\n",
       "         0.81985576, -0.7802652 ],\n",
       "       [-0.32716767, -1.02292666, -0.98353566, ...,  1.24374445,\n",
       "         0.26616225, -0.39550477],\n",
       "       [-1.45216528, -0.75831903, -0.61675398, ..., -0.13453654,\n",
       "        -0.6554412 ,  0.47856494]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X #exemple X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=58,kernel_initializer='normal', activation='relu'))#\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(6,kernel_initializer='normal', activation='softmax'))# a revoir la fct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27310/27310 [==============================] - 3s 98us/step - loss: 0.7090 - acc: 0.7277\n",
      "Epoch 2/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.5903 - acc: 0.7719\n",
      "Epoch 3/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.5546 - acc: 0.7865\n",
      "Epoch 4/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.5321 - acc: 0.7959\n",
      "Epoch 5/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.5167 - acc: 0.8012\n",
      "Epoch 6/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.5036 - acc: 0.8084\n",
      "Epoch 7/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.4946 - acc: 0.8119\n",
      "Epoch 8/100\n",
      "27310/27310 [==============================] - 2s 73us/step - loss: 0.4821 - acc: 0.8160\n",
      "Epoch 9/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.4742 - acc: 0.8179\n",
      "Epoch 10/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4655 - acc: 0.8205\n",
      "Epoch 11/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4587 - acc: 0.8254\n",
      "Epoch 12/100\n",
      "27310/27310 [==============================] - 2s 80us/step - loss: 0.4547 - acc: 0.8272\n",
      "Epoch 13/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.4463 - acc: 0.8305\n",
      "Epoch 14/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4415 - acc: 0.8324\n",
      "Epoch 15/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4369 - acc: 0.8358\n",
      "Epoch 16/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4328 - acc: 0.8343\n",
      "Epoch 17/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4273 - acc: 0.8363\n",
      "Epoch 18/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.4242 - acc: 0.8397\n",
      "Epoch 19/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.4203 - acc: 0.8418\n",
      "Epoch 20/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.4154 - acc: 0.8420\n",
      "Epoch 21/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.4114 - acc: 0.8427\n",
      "Epoch 22/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.4104 - acc: 0.8448\n",
      "Epoch 23/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.4067 - acc: 0.8438\n",
      "Epoch 24/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.4028 - acc: 0.8458\n",
      "Epoch 25/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.3992 - acc: 0.8486\n",
      "Epoch 26/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.3954 - acc: 0.8505\n",
      "Epoch 27/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3933 - acc: 0.8512\n",
      "Epoch 28/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3866 - acc: 0.8529\n",
      "Epoch 29/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3858 - acc: 0.8516\n",
      "Epoch 30/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.3824 - acc: 0.8536\n",
      "Epoch 31/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3794 - acc: 0.8582\n",
      "Epoch 32/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3769 - acc: 0.8562\n",
      "Epoch 33/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3747 - acc: 0.8575\n",
      "Epoch 34/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3690 - acc: 0.8592\n",
      "Epoch 35/100\n",
      "27310/27310 [==============================] - 2s 81us/step - loss: 0.3666 - acc: 0.8611\n",
      "Epoch 36/100\n",
      "27310/27310 [==============================] - 2s 81us/step - loss: 0.3663 - acc: 0.8598\n",
      "Epoch 37/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3609 - acc: 0.8632\n",
      "Epoch 38/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.3606 - acc: 0.8633\n",
      "Epoch 39/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3563 - acc: 0.8633\n",
      "Epoch 40/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3556 - acc: 0.8629\n",
      "Epoch 41/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3540 - acc: 0.8657\n",
      "Epoch 42/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3487 - acc: 0.8677\n",
      "Epoch 43/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3477 - acc: 0.8674\n",
      "Epoch 44/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3456 - acc: 0.8680\n",
      "Epoch 45/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3422 - acc: 0.8698\n",
      "Epoch 46/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.3398 - acc: 0.8705\n",
      "Epoch 47/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3365 - acc: 0.8718\n",
      "Epoch 48/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.3340 - acc: 0.8727\n",
      "Epoch 49/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3313 - acc: 0.8729\n",
      "Epoch 50/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.3287 - acc: 0.8736\n",
      "Epoch 51/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.3267 - acc: 0.8755\n",
      "Epoch 52/100\n",
      "27310/27310 [==============================] - 2s 80us/step - loss: 0.3246 - acc: 0.8750\n",
      "Epoch 53/100\n",
      "27310/27310 [==============================] - 2s 82us/step - loss: 0.3214 - acc: 0.8778\n",
      "Epoch 54/100\n",
      "27310/27310 [==============================] - 2s 84us/step - loss: 0.3188 - acc: 0.8778\n",
      "Epoch 55/100\n",
      "27310/27310 [==============================] - 2s 86us/step - loss: 0.3165 - acc: 0.8785\n",
      "Epoch 56/100\n",
      "27310/27310 [==============================] - 2s 81us/step - loss: 0.3131 - acc: 0.8807\n",
      "Epoch 57/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.3099 - acc: 0.8830\n",
      "Epoch 58/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.3090 - acc: 0.8835\n",
      "Epoch 59/100\n",
      "27310/27310 [==============================] - 2s 81us/step - loss: 0.3055 - acc: 0.8826\n",
      "Epoch 60/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.3026 - acc: 0.8840\n",
      "Epoch 61/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.2999 - acc: 0.8851\n",
      "Epoch 62/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2986 - acc: 0.8855\n",
      "Epoch 63/100\n",
      "27310/27310 [==============================] - 2s 84us/step - loss: 0.2956 - acc: 0.8882\n",
      "Epoch 64/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.2933 - acc: 0.8893\n",
      "Epoch 65/100\n",
      "27310/27310 [==============================] - 2s 80us/step - loss: 0.2906 - acc: 0.8895\n",
      "Epoch 66/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.2891 - acc: 0.8899\n",
      "Epoch 67/100\n",
      "27310/27310 [==============================] - 2s 80us/step - loss: 0.2858 - acc: 0.8907\n",
      "Epoch 68/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.2828 - acc: 0.8918\n",
      "Epoch 69/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.2803 - acc: 0.8931\n",
      "Epoch 70/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.2794 - acc: 0.8937\n",
      "Epoch 71/100\n",
      "27310/27310 [==============================] - 2s 81us/step - loss: 0.2769 - acc: 0.8947\n",
      "Epoch 72/100\n",
      "27310/27310 [==============================] - 2s 85us/step - loss: 0.2716 - acc: 0.8960\n",
      "Epoch 73/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.2707 - acc: 0.8966\n",
      "Epoch 74/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2684 - acc: 0.8990\n",
      "Epoch 75/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2640 - acc: 0.8990\n",
      "Epoch 76/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2630 - acc: 0.9002\n",
      "Epoch 77/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2606 - acc: 0.8998\n",
      "Epoch 78/100\n",
      "27310/27310 [==============================] - 2s 77us/step - loss: 0.2583 - acc: 0.9012\n",
      "Epoch 79/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2549 - acc: 0.9044\n",
      "Epoch 80/100\n",
      "27310/27310 [==============================] - 2s 78us/step - loss: 0.2536 - acc: 0.9040\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2495 - acc: 0.9048\n",
      "Epoch 82/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.2479 - acc: 0.9053\n",
      "Epoch 83/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2468 - acc: 0.9065\n",
      "Epoch 84/100\n",
      "27310/27310 [==============================] - 2s 82us/step - loss: 0.2438 - acc: 0.9074\n",
      "Epoch 85/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2418 - acc: 0.9083\n",
      "Epoch 86/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2404 - acc: 0.9086\n",
      "Epoch 87/100\n",
      "27310/27310 [==============================] - 2s 79us/step - loss: 0.2366 - acc: 0.9108\n",
      "Epoch 88/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2346 - acc: 0.9127\n",
      "Epoch 89/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2303 - acc: 0.9126\n",
      "Epoch 90/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2277 - acc: 0.9137\n",
      "Epoch 91/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2268 - acc: 0.9159\n",
      "Epoch 92/100\n",
      "27310/27310 [==============================] - 2s 76us/step - loss: 0.2255 - acc: 0.9155\n",
      "Epoch 93/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.2212 - acc: 0.9189\n",
      "Epoch 94/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.2204 - acc: 0.9160\n",
      "Epoch 95/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2172 - acc: 0.9180\n",
      "Epoch 96/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.2154 - acc: 0.9192\n",
      "Epoch 97/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2149 - acc: 0.9187\n",
      "Epoch 98/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2112 - acc: 0.9200\n",
      "Epoch 99/100\n",
      "27310/27310 [==============================] - 2s 75us/step - loss: 0.2076 - acc: 0.9225\n",
      "Epoch 100/100\n",
      "27310/27310 [==============================] - 2s 74us/step - loss: 0.2084 - acc: 0.9226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a407d29b0>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, y_encode_binaire, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27310/27310 [==============================] - 1s 37us/step\n",
      "\n",
      "acc: 92.95%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y_encode_binaire)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27310,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3bf3d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict_classes(X)\n",
    "classes = encoder.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classes.shape)\n",
    "sns.countplot(classes,label=\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2b9e9780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6828,)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(y_test,label=\"Count\")\n",
    "plt.show()\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
